usewandb: true
random_seed: love ytq forever
device: null

action:
  type: train
  args:
    n_epoch: 1000000
    ckpt_save_name: checkpoints/native-{epoch:05d}.pt
    per_epoch_save: 1000
    betas: [0.9, 0.98]
    eps: 1e-9
    scheduler_factor: 0.8
    scheduler_warmup: 200
    evaluetaion: true
    n_validation_sample: 20
    main_loss_ratio: 1

######### For evaluation ONLY, the paramerters for latentcode decoder.
# cuz, I wrongly save the state_dict of the decoder instead of the whole pickled model.
# TODO: save the whole model.
n_latent_code_validation_samples: 10000
latent_decoder:
  type: null
  ckpt_path: /home/shuyumo/research/GAO/point2sdf/ckpt/sgd-e-d-0.2-432-0.9829541444778442.ckpt
  args:
    z_dim: 128
    c_dim: 0
    emb_sigma: 0.1
    leaky: 0.02
######### END

# Dataset & Dataloader config
dataset:
  type: redis
  args:
    db: 0
    port: 6379
    host: localhost
    fix_length: 5

dataloader:
  args:
    batch_size: 16
    num_workers: 4
    shuffle: true

part_structure:
  non_latent_info:
    origin: 3
    direction: 3
    bounds: 6
    tran: 3
    limit: 4
  latent_info:
    latent: 128
  vocab_size: 16384 # >>> 2048 * 8

# Global Model Parameters
model_parameter:
  tokenizer_hidden_dim: 512
  tokenizer_leaky_relu: 0.1

  n_head: 8
  n_layer: 8
  d_model: 512
  max_part_len_embedding: 128
  decoder_dropout: 0.1
  expanded_d_model: 4096

  ffd_hidden_dim: 4096
  ffd_dropout: 0.1

## Module Parameter Config.
## It will read from above parameter.
## No need to change here.
tokenizer:
  type: NativeMLPTokenizer
  args:
    input_structure: part_structure
    d_model: model_parameter.d_model
    hidden_dim: model_parameter.tokenizer_hidden_dim
    latent_code_dim: part_structure.latent_info.latent
    leaky_relu: model_parameter.tokenizer_leaky_relu
    drop_out: model_parameter.decoder_dropout

position_embedding:
  type: NativeCatPositionEmbedding
  args:
    d_model: model_parameter.d_model
    max_len: model_parameter.max_part_len_embedding
    device: device

g_embedding:
  type: NativeGEmbedding
  args:
    vocab_size: part_structure.vocab_size
    d_model: model_parameter.d_model

untokenizer:
  type: NativeMLPUnTokenizer
  args:
    input_structure: part_structure
    d_model: model_parameter.d_model
    expanded_d_model: model_parameter.expanded_d_model
    latent_code_dim: part_structure.latent_info.latent
    dropout: model_parameter.decoder_dropout

decoder:
  type: NativeDecoder
  args:
    config: '.'
    n_layer: model_parameter.n_layer
    device: device
  layer_arges:
    n_head: model_parameter.n_head
    d_model: model_parameter.d_model
    dropout: model_parameter.decoder_dropout

feedforward: # TODO: try KAN.
  args:
    d_model: model_parameter.d_model
    hidden_dim: model_parameter.ffd_hidden_dim
    dropout: model_parameter.ffd_dropout


